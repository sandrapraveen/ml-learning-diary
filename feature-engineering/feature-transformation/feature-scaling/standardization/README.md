# Standardization in Machine Learning 

This repository contains my notebook **`standardization.ipynb`**, where I explore the concept of **feature standardization** â€” one of the most important preprocessing steps in Machine Learning.

---

## ğŸ“Œ Whatâ€™s inside?
In the notebook, I cover:
- âœ¨ Why standardization is important  
- ğŸ“Š How to use `StandardScaler` from scikit-learn  
- ğŸ” The difference between `fit`, `transform`, and `fit_transform`  
- âš–ï¸ Effect of standardization on different ML models  
- ğŸ§ª Experiments comparing models **before vs. after scaling**

---

## ğŸ› ï¸ Tech Stack
- Python ğŸ  
- Pandas, NumPy  
- Scikit-learn  
- Jupyter Notebook  

---

## ğŸ”¬ Key Learnings
- Decision Trees and Random Forests are **scale-invariant** ğŸŒ³  
- Logistic Regression, SVM, KNN, and Neural Networks **need scaling** to perform well  
- Standardization makes training more **stable and faster**  
- Avoid **data leakage**: use `fit_transform()` on the training set, and only `transform()` on the test set  

---
